name: Test Embeddings Build (Temporary)

# Temporary workflow to test llama.cpp builds on all platforms
# DELETE THIS FILE before merging to main

on:
  push:
    branches:
      - feat/embeddings
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build-backend:
    strategy:
      fail-fast: false
      matrix:
        config:
          - os: ubuntu-latest
            name: linux-x64
            daemon_name: x86_64-unknown-linux-gnu
          - os: macos-15-large
            name: macos-x64
            daemon_name: x86_64-apple-darwin
          - os: macos-15-xlarge
            name: macos-arm64
            daemon_name: aarch64-apple-darwin
          - os: windows-2025
            name: windows-x64
            daemon_name: x86_64-pc-windows-msvc

    runs-on: ${{ matrix.config.os }}
    name: Build ${{ matrix.config.name }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Cache GGUF model
        uses: actions/cache@v4
        with:
          path: backend/llm/backends/llamacpp/models/*.gguf
          key: gguf-model-v2
          enableCrossOsArchive: true

      - name: Download GGUF model (Unix)
        if: matrix.config.os != 'windows-2025'
        run: |
          if [ ! -f backend/llm/backends/llamacpp/models/paraphrase-multilingual-MiniLM-L12-118M-v2-Q8_0.gguf ]; then
            mkdir -p backend/llm/backends/llamacpp/models
            curl -fSL -o backend/llm/backends/llamacpp/models/paraphrase-multilingual-MiniLM-L12-118M-v2-Q8_0.gguf \
              https://seedllmmodels.s3.us-east-1.amazonaws.com/embedding/paraphrase-multilingual-MiniLM-L12-118M-v2-Q8_0.gguf
          fi

      - name: Download GGUF model (Windows)
        if: matrix.config.os == 'windows-2025'
        shell: powershell
        run: |
          $modelPath = "backend/llm/backends/llamacpp/models/paraphrase-multilingual-MiniLM-L12-118M-v2-Q8_0.gguf"
          if (!(Test-Path $modelPath)) {
            New-Item -ItemType Directory -Force -Path "backend/llm/backends/llamacpp/models"
            Invoke-WebRequest -Uri "https://seedllmmodels.s3.us-east-1.amazonaws.com/embedding/paraphrase-multilingual-MiniLM-L12-118M-v2-Q8_0.gguf" -OutFile $modelPath
          }

      - uses: ./.github/actions/ci-setup
        with:
          matrix-os: ${{ matrix.config.os }}

      - name: Build seed-daemon (Unix)
        if: matrix.config.os != 'windows-2025'
        run: |
          go build -tags gpu -o seed-daemon-${{ matrix.config.daemon_name }} ./backend/cmd/seed-daemon
          ls -la seed-daemon-*
        env:
          CGO_ENABLED: 1
          LIBRARY_PATH: ${{ github.workspace }}/backend/util/llama-go
          C_INCLUDE_PATH: ${{ github.workspace }}/backend/util/llama-go

      - name: Build seed-daemon (Windows)
        if: matrix.config.os == 'windows-2025'
        shell: bash
        run: |
          go build -tags gpu -o seed-daemon-${{ matrix.config.daemon_name }}.exe ./backend/cmd/seed-daemon
          ls -la seed-daemon-*
        env:
          CGO_ENABLED: 1
          LIBRARY_PATH: ${{ github.workspace }}/backend/util/llama-go
          C_INCLUDE_PATH: ${{ github.workspace }}/backend/util/llama-go

      - name: Verify binary
        run: |
          echo "Build successful for ${{ matrix.config.name }}"
          file seed-daemon-* || true
