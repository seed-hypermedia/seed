name: Test Embeddings Build (Temporary)

# Temporary workflow to test llama.cpp builds on all platforms
# DELETE THIS FILE before merging to main

on:
  push:
    branches:
      - feat/embeddings
  workflow_dispatch:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build-backend:
    strategy:
      fail-fast: false
      matrix:
        config:
          - os: ubuntu-latest
            name: linux-x64
            build_type: vulkan
            daemon_name: x86_64-unknown-linux-gnu
          - os: macos-14
            name: macos-x64
            build_type: metal
            daemon_name: x86_64-apple-darwin
          - os: macos-latest
            name: macos-arm64
            build_type: metal
            daemon_name: aarch64-apple-darwin
          - os: windows-2025
            name: windows-x64
            build_type: vulkan
            daemon_name: x86_64-pc-windows-msvc

    runs-on: ${{ matrix.config.os }}
    name: Build ${{ matrix.config.name }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: "1.25.4"

      - name: Install build dependencies (Linux)
        if: matrix.config.os == 'ubuntu-latest'
        run: |
          sudo apt-get update
          sudo apt-get install -y cmake g++ libvulkan-dev glslc libgomp1

      - name: Install build dependencies (macOS)
        if: startsWith(matrix.config.os, 'macos')
        run: |
          brew install cmake

      - name: Install build dependencies (Windows)
        if: matrix.config.os == 'windows-2025'
        run: |
          choco install vulkan-sdk -y
          choco install cmake -y
          choco install mingw -y
        shell: powershell

      - name: Build llama.cpp (Linux)
        if: matrix.config.os == 'ubuntu-latest'
        run: |
          cd backend/util/llama-go
          BUILD_TYPE=${{ matrix.config.build_type }} CMAKE_ARGS="-DBUILD_SHARED_LIBS=OFF" make libbinding.a
          ls -la *.a

      - name: Build llama.cpp (macOS)
        if: startsWith(matrix.config.os, 'macos')
        run: |
          cd backend/util/llama-go
          BUILD_TYPE=${{ matrix.config.build_type }} CMAKE_ARGS="-DBUILD_SHARED_LIBS=OFF" make libbinding.a
          ls -la *.a

      - name: Build llama.cpp (Windows)
        if: matrix.config.os == 'windows-2025'
        shell: bash
        run: |
          cd backend/util/llama-go
          BUILD_TYPE=${{ matrix.config.build_type }} CMAKE_ARGS="-DBUILD_SHARED_LIBS=OFF" make libbinding.a
          ls -la *.a

      - name: Download GGUF model (Unix)
        if: matrix.config.os != 'windows-2025'
        run: |
          if [ ! -f backend/llm/backends/llamacpp/models/paraphrase-multilingual-MiniLM-L12-118M-v2-Q8_0.gguf ]; then
            mkdir -p backend/llm/backends/llamacpp/models
            curl -fSL -o backend/llm/backends/llamacpp/models/paraphrase-multilingual-MiniLM-L12-118M-v2-Q8_0.gguf \
              https://seedllmmodels.s3.us-east-1.amazonaws.com/embedding/paraphrase-multilingual-MiniLM-L12-118M-v2-Q8_0.gguf
          fi

      - name: Download GGUF model (Windows)
        if: matrix.config.os == 'windows-2025'
        shell: powershell
        run: |
          $modelPath = "backend/llm/backends/llamacpp/models/paraphrase-multilingual-MiniLM-L12-118M-v2-Q8_0.gguf"
          if (!(Test-Path $modelPath)) {
            New-Item -ItemType Directory -Force -Path "backend/llm/backends/llamacpp/models"
            Invoke-WebRequest -Uri "https://seedllmmodels.s3.us-east-1.amazonaws.com/embedding/paraphrase-multilingual-MiniLM-L12-118M-v2-Q8_0.gguf" -OutFile $modelPath
          }

      - name: Build seed-daemon (Unix)
        if: matrix.config.os != 'windows-2025'
        run: |
          go build -tags gpu -o seed-daemon-${{ matrix.config.daemon_name }} ./backend/cmd/seed-daemon
          ls -la seed-daemon-*
        env:
          CGO_ENABLED: 1
          LIBRARY_PATH: ${{ github.workspace }}/backend/util/llama-go
          C_INCLUDE_PATH: ${{ github.workspace }}/backend/util/llama-go

      - name: Build seed-daemon (Windows)
        if: matrix.config.os == 'windows-2025'
        run: |
          go build -tags gpu -o seed-daemon-${{ matrix.config.daemon_name }}.exe ./backend/cmd/seed-daemon
          ls -la seed-daemon-*
        shell: bash
        env:
          CGO_ENABLED: 1
          LIBRARY_PATH: ${{ github.workspace }}/backend/util/llama-go
          C_INCLUDE_PATH: ${{ github.workspace }}/backend/util/llama-go

      - name: Verify binary
        run: |
          echo "Build successful for ${{ matrix.config.name }}"
          file seed-daemon-* || true
