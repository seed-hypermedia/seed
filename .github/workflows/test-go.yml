name: Test Go

on:
  push:
    branches:
      - main
    paths:
      - ".github/workflows/test-go.yml"
      - "go.mod"
      - "backend/**"

  pull_request:
    paths:
      - ".github/workflows/test-go.yml"
      - "go.mod"
      - "backend/**"
    branches-ignore:
      - "renovate/**"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  test-go:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          submodules: recursive

      - name: Cache GGUF model
        uses: actions/cache@v4
        with:
          path: backend/llm/backends/llamacpp/models/*.gguf
          key: gguf-model-granite-v2
          enableCrossOsArchive: true

      - name: Download GGUF model
        run: |
          if [ ! -f backend/llm/backends/llamacpp/models/granite-embedding-107m-multilingual-Q8_0.gguf ]; then
            mkdir -p backend/llm/backends/llamacpp/models
            curl -fSL -o backend/llm/backends/llamacpp/models/granite-embedding-107m-multilingual-Q8_0.gguf \
              "https://huggingface.co/keisuke-miyako/granite-embedding-107m-multilingual-gguf-q8_0/resolve/main/granite-embedding-107m-multilingual-Q8_0.gguf?download=true"
          fi

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: "1.25.4"

      - name: Install build dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y cmake g++

      - name: Build llama.cpp (CPU-only for tests)
        run: |
          cd backend/util/llama-go
          # Tests run without GPU hardware, so we build CPU-only
          CMAKE_ARGS="-DBUILD_SHARED_LIBS=OFF -DGGML_VULKAN=OFF -DGGML_METAL=OFF -DGGML_CUDA=OFF -DGGML_HIP=OFF -DGGML_SYCL=OFF -DGGML_BLAS=OFF" make libbinding.a

      - name: Run tests
        run: go test -tags cpu --count 1 ./backend/...
        env:
          CGO_ENABLED: 1
          LIBRARY_PATH: ${{ github.workspace }}/backend/util/llama-go
          C_INCLUDE_PATH: ${{ github.workspace }}/backend/util/llama-go
          LLAMA_LOG: error

      # Run tests again with the race-detector.
      # Using the same job to reuse the build cache.
      - name: Run tests with race detector
        run: go test -tags cpu --count 1 -race ./backend/...
        env:
          CGO_ENABLED: 1
          LIBRARY_PATH: ${{ github.workspace }}/backend/util/llama-go
          C_INCLUDE_PATH: ${{ github.workspace }}/backend/util/llama-go
          LLAMA_LOG: error
